{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1ac931ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "589ecf36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "df22f4c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "16ef83fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/wk247/workspace/stitching/src/transformers/__init__.py'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformers.__file__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa983d92",
   "metadata": {},
   "source": [
    "### Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "135a235c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "# vocabs are identical for small and large\n",
    "tokenizer = BertTokenizer('../vocab.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd7b1646",
   "metadata": {},
   "source": [
    "### Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3456820e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at prajjwal1/bert-mini were not used when initializing BertModel: ['cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at prajjwal1/bert-small were not used when initializing BertModel: ['cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# from transformers\n",
    "from transformers import AutoModel\n",
    "\n",
    "small_model = AutoModel.from_pretrained(\"prajjwal1/bert-mini\").to(device)\n",
    "large_model = AutoModel.from_pretrained(\"prajjwal1/bert-small\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "50d7c514",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define small model 1, 2\n",
    "import copy\n",
    "\n",
    "small_model1 = small_model\n",
    "small_model2 = copy.deepcopy(small_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e8136de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change configs to return intermediate outputs\n",
    "# https://huggingface.co/docs/transformers/model_doc/bert#transformers.BertForPreTraining.forward\n",
    "small_model.config.output_hidden_states=True\n",
    "small_model.config.output_attentions=True\n",
    "large_model.config.output_hidden_states=True\n",
    "large_model.config.output_attentions=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fa2950de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "small model\n",
      "- num_parameters: 11170560\n",
      "- hidden_size: 256\n",
      "- num_attention_heads: 4\n",
      "- num_hidden_layers: 4\n",
      "\n",
      "large model\n",
      "- num_parameters: 28763648\n",
      "- hidden_size: 512\n",
      "- num_attention_heads: 8\n",
      "- num_hidden_layers: 4\n"
     ]
    }
   ],
   "source": [
    "# model configs\n",
    "print(\"small model\")\n",
    "print(f'- num_parameters: {small_model.num_parameters()}')\n",
    "print(f\"- hidden_size: {small_model.config.hidden_size}\")\n",
    "print(f\"- num_attention_heads: {small_model.config.num_attention_heads}\")\n",
    "print(f\"- num_hidden_layers: {small_model.config.num_hidden_layers}\")\n",
    "print()\n",
    "\n",
    "print(\"large model\")\n",
    "print(f'- num_parameters: {large_model.num_parameters()}')\n",
    "print(f\"- hidden_size: {large_model.config.hidden_size}\")\n",
    "print(f\"- num_attention_heads: {large_model.config.num_attention_heads}\")\n",
    "print(f\"- num_hidden_layers: {large_model.config.num_hidden_layers}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2674e280",
   "metadata": {},
   "source": [
    "### Stitched config/model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dbcc341b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StitchedBertConfig {\n",
       "  \"_name_or_path\": \"prajjwal1/bert-mini\",\n",
       "  \"attention_probs_dropout_prob\": 0.1,\n",
       "  \"classifier_dropout\": null,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.1,\n",
       "  \"hidden_size\": 256,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 1024,\n",
       "  \"layer_norm_eps\": 1e-12,\n",
       "  \"max_position_embeddings\": 512,\n",
       "  \"model_type\": \"stitched_bert\",\n",
       "  \"num_attention_heads\": 4,\n",
       "  \"num_hidden_layers\": 4,\n",
       "  \"output_attentions\": true,\n",
       "  \"output_hidden_states\": true,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"position_embedding_type\": \"absolute\",\n",
       "  \"stitch_hidden_size\": 512,\n",
       "  \"stitch_intermediate_size\": 2048,\n",
       "  \"transformers_version\": \"4.17.0\",\n",
       "  \"type_vocab_size\": 2,\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 30522\n",
       "}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import StitchedBertConfig\n",
    "\n",
    "stitched_config = StitchedBertConfig(**small_model.config.to_dict())\n",
    "stitched_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c0926ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import StitchedBertModel\n",
    "\n",
    "stitched_model = StitchedBertModel(stitched_config).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23af7847",
   "metadata": {},
   "source": [
    "#### 1. forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "999a979e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# forward\n",
    "text = \"Example input\"\n",
    "encoded_input = tokenizer(text, return_tensors='pt').to(device)\n",
    "# small_output = small_model(**encoded_input, return_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0fbd45bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = stitched_model(**encoded_input, return_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "057f6b85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last_hidden_state: torch.Size([1, 4, 512])\n",
      "pooler_output: torch.Size([1, 512])\n"
     ]
    }
   ],
   "source": [
    "# output shapes\n",
    "print(\"last_hidden_state:\", output[\"last_hidden_state\"].shape)\n",
    "print(\"pooler_output:\", output[\"pooler_output\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b83f52f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# includes the last hidden states\n",
    "assert torch.isclose(output.hidden_states[-1], output.last_hidden_state).all()\n",
    "\n",
    "len(output.hidden_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3b3fae7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Attentions weights after the attention softmax, used to compute the weighted average in the self-attention heads.\n",
    "len(output.attentions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a1d705ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 4, 4, 4])\n",
      "torch.Size([2, 4, 4, 4])\n",
      "torch.Size([2, 4, 4, 4])\n",
      "torch.Size([2, 4, 4, 4])\n"
     ]
    }
   ],
   "source": [
    "# attentions\n",
    "for attn in output.attentions:\n",
    "    print(attn.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ace1b0e",
   "metadata": {},
   "source": [
    "#### 2. copy modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ab03982a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0f5ca7dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy embeddings\n",
    "# NOTE: check if deepcopy is okay\n",
    "stitched_model.embeddings1 = copy.deepcopy(small_model1.embeddings)\n",
    "stitched_model.embeddings2 = copy.deepcopy(small_model2.embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8258e468",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert torch.isclose(small_model1.embeddings.word_embeddings.weight, stitched_model.embeddings1.word_embeddings.weight).all()\n",
    "assert torch.isclose(small_model2.embeddings.word_embeddings.weight, stitched_model.embeddings2.word_embeddings.weight).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "097d7f46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bias\n"
     ]
    }
   ],
   "source": [
    "l1 = torch.nn.Linear(10, 100, bias=True)\n",
    "l2 = torch.nn.Linear(10, 100, bias=True)\n",
    "if all((l1.bias !=  None, l2.bias != None)) :\n",
    "    print(\"bias\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "817971e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all((1, 1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ddc79a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_linear(tgt, src1, src2):\n",
    "    \"\"\"\n",
    "    all: torch.nn.Linear\n",
    "    \"\"\"\n",
    "    # check if bias exists\n",
    "    assert None not in (tgt.bias, src1.bias, src2.bias) or not any((tgt.bias, src1.bias, src2.bias))\n",
    "    \n",
    "    tgt_out_dim, tgt_in_dim = tgt.weight.size()\n",
    "    src1_out_dim, src1_in_dim = src1.weight.size()\n",
    "    src2_out_dim, src2_in_dim = src2.weight.size()\n",
    "\n",
    "    assert tgt_out_dim == src1_out_dim + src2_out_dim\n",
    "    assert tgt_in_dim == src1_in_dim + src2_in_dim\n",
    "\n",
    "    # NOTE: check indexing\n",
    "    tgt.weight.detach()[:src1_out_dim, :src1_in_dim] = src1.weight.detach()\n",
    "    tgt.weight.detach()[-src2_out_dim:, -src2_in_dim:] = src2.weight.detach()\n",
    "    \n",
    "    if tgt.bias is not None:\n",
    "        # copy bias\n",
    "        tgt.bias.detach()[:src1_out_dim] = src1.bias.detach()\n",
    "        tgt.bias.detach()[-src2_out_dim:] = src2.bias.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "43d130cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_layernorm(tgt, src1, src2):\n",
    "    \"\"\"\n",
    "    all: torch.nn.modules.normalization.LayerNorm\n",
    "    \"\"\"\n",
    "    tgt_dim, src1_dim, src2_dim = tgt.weight.size(0), src1.weight.size(0), src2.weight.size(0)\n",
    "    assert tgt_dim == src1_dim + src2_dim\n",
    "\n",
    "    # # NOTE: check indexing\n",
    "    # copy weights\n",
    "    tgt.weight.detach()[:src1_dim] = src1.weight.detach()\n",
    "    tgt.weight.detach()[-src2_dim:] = src2.weight.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f9745ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy within layers\n",
    "for layer_st, layer_1, layer_2 in zip(stitched_model.encoder.layer, small_model1.encoder.layer, small_model2.encoder.layer):\n",
    "    assert type(layer_st.attention1) == type(layer_1.attention)\n",
    "    assert type(layer_st.attention2) == type(layer_2.attention)\n",
    "\n",
    "    # copy attention modules\n",
    "    layer_st.attention1 = copy.deepcopy(layer_1.attention)\n",
    "    layer_st.attention2 = copy.deepcopy(layer_2.attention)\n",
    "\n",
    "    # copy intermediate ffn\n",
    "    copy_linear(layer_st.intermediate.dense, layer_1.intermediate.dense, layer_2.intermediate.dense)\n",
    "\n",
    "    # copy output ffn\n",
    "    copy_linear(layer_st.output.dense, layer_1.output.dense, layer_2.output.dense)\n",
    "    copy_layernorm(layer_st.output.LayerNorm, layer_1.output.LayerNorm, layer_2.output.LayerNorm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3cfde697",
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy pooler\n",
    "copy_linear(stitched_model.pooler.dense, small_model1.pooler.dense, small_model2.pooler.dense)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2061b2ed",
   "metadata": {},
   "source": [
    "# TODO: copy sanity check, incorporate to bertmodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4007ee88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertLayer(\n",
       "  (attention): BertAttention(\n",
       "    (self): BertSelfAttention(\n",
       "      (query): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (key): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (value): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (output): BertSelfOutput(\n",
       "      (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (intermediate): BertIntermediate(\n",
       "    (dense): Linear(in_features=512, out_features=2048, bias=True)\n",
       "  )\n",
       "  (output): BertOutput(\n",
       "    (dense): Linear(in_features=2048, out_features=512, bias=True)\n",
       "    (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# nonlinear activation is inside BertIntermediate\n",
    "large_model.encoder.layer[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acfdb46d",
   "metadata": {},
   "outputs": [],
   "source": [
    "large_model.generate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b3af3a7e",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "int() argument must be a string, a bytes-like object or a number, not 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/wk247/workspace/lm_stitching/notebooks/bert.ipynb Cell 13\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B128.84.100.162/home/wk247/workspace/lm_stitching/notebooks/bert.ipynb#X32sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m tokenizer\u001b[39m.\u001b[39;49mdecode(output[\u001b[39m\"\u001b[39;49m\u001b[39mpooler_output\u001b[39;49m\u001b[39m\"\u001b[39;49m])\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:3292\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.decode\u001b[0;34m(self, token_ids, skip_special_tokens, clean_up_tokenization_spaces, **kwargs)\u001b[0m\n\u001b[1;32m   3289\u001b[0m \u001b[39m# Convert inputs to python lists\u001b[39;00m\n\u001b[1;32m   3290\u001b[0m token_ids \u001b[39m=\u001b[39m to_py_obj(token_ids)\n\u001b[0;32m-> 3292\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_decode(\n\u001b[1;32m   3293\u001b[0m     token_ids\u001b[39m=\u001b[39;49mtoken_ids,\n\u001b[1;32m   3294\u001b[0m     skip_special_tokens\u001b[39m=\u001b[39;49mskip_special_tokens,\n\u001b[1;32m   3295\u001b[0m     clean_up_tokenization_spaces\u001b[39m=\u001b[39;49mclean_up_tokenization_spaces,\n\u001b[1;32m   3296\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m   3297\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/tokenization_utils.py:929\u001b[0m, in \u001b[0;36mPreTrainedTokenizer._decode\u001b[0;34m(self, token_ids, skip_special_tokens, clean_up_tokenization_spaces, spaces_between_special_tokens, **kwargs)\u001b[0m\n\u001b[1;32m    919\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_decode\u001b[39m(\n\u001b[1;32m    920\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    921\u001b[0m     token_ids: List[\u001b[39mint\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    925\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs\n\u001b[1;32m    926\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mstr\u001b[39m:\n\u001b[1;32m    927\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_decode_use_source_tokenizer \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39muse_source_tokenizer\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m--> 929\u001b[0m     filtered_tokens \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconvert_ids_to_tokens(token_ids, skip_special_tokens\u001b[39m=\u001b[39;49mskip_special_tokens)\n\u001b[1;32m    931\u001b[0m     \u001b[39m# To avoid mixing byte-level and unicode for byte-level BPT\u001b[39;00m\n\u001b[1;32m    932\u001b[0m     \u001b[39m# we need to build string separately for added tokens and byte-level tokens\u001b[39;00m\n\u001b[1;32m    933\u001b[0m     \u001b[39m# cf. https://github.com/huggingface/transformers/issues/1133\u001b[39;00m\n\u001b[1;32m    934\u001b[0m     sub_texts \u001b[39m=\u001b[39m []\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/tokenization_utils.py:904\u001b[0m, in \u001b[0;36mPreTrainedTokenizer.convert_ids_to_tokens\u001b[0;34m(self, ids, skip_special_tokens)\u001b[0m\n\u001b[1;32m    902\u001b[0m tokens \u001b[39m=\u001b[39m []\n\u001b[1;32m    903\u001b[0m \u001b[39mfor\u001b[39;00m index \u001b[39min\u001b[39;00m ids:\n\u001b[0;32m--> 904\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mint\u001b[39;49m(index)\n\u001b[1;32m    905\u001b[0m     \u001b[39mif\u001b[39;00m skip_special_tokens \u001b[39mand\u001b[39;00m index \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mall_special_ids:\n\u001b[1;32m    906\u001b[0m         \u001b[39mcontinue\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: int() argument must be a string, a bytes-like object or a number, not 'list'"
     ]
    }
   ],
   "source": [
    "tokenizer.decode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30eae786",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('stitching')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "0c6e9ac6b7409738743aa5d1e2dd5d1b900da62de1681e3288e7c2b10d8e3740"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
